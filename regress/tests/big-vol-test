#!/bin/sh
#
# Create a big Volume > 5 GB and backup to it to test disk
#   seeking on big volumes.  We cheat and artifically grow 
#   the volume.
#
TestName="big-vol-test"
JobName=bigvol
. scripts/functions

scripts/cleanup
scripts/copy-test-confs
cwd=`pwd`
echo "${cwd}/build" >${cwd}/tmp/file-list

change_jobname CompressedTest $JobName
start_test

cat <<END_OF_DATA >${cwd}/tmp/bconcmds
@$out /dev/null
messages
@$out ${cwd}/tmp/log1.out
@#setdebug level=100 storage=File
label storage=File volume=TestVolume001
run job=$JobName yes
wait
messages
quit
END_OF_DATA

run_bacula
check_for_zombie_jobs storage=File

#
# Now increase the size of the Volume using gigaslam
#
cd ${cwd}/tmp
# make big file
size=5200000000
${cwd}/build/src/tools/grow TestVolume001 ${size}
if [ $? != 0 ]; then
   echo "Execute of ${cwd}/build/src/tools/grow failed."
   exit 1
fi
cd ${cwd}

cat <<END_OF_DATA >${cwd}/tmp/bconcmds
@$out /dev/null
messages
@$out ${cwd}/tmp/log1.out
sql
UPDATE Media SET Volbytes=${size} WHERE VolumeName='TestVolume001';

llist volume=TestVolume001
@# Now do another full save with big Volume
run level=Full job=$JobName yes
wait
messages
@# 
@# now do a restore
@#
sql
SELECT * FROM JobMedia;

@$out ${cwd}/tmp/log2.out
restore where=${cwd}/tmp/bacula-restores storage=File select all done
yes
wait
messages
quit
END_OF_DATA

run_bconsole
check_for_zombie_jobs storage=File
stop_bacula

check_two_logs
check_restore_diff
end_test
# Get rid of big files
rm -f ${cwd}/tmp/TestVolume001
